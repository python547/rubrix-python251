{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uses cases \n",
    "\n",
    "See [this issue](https://gitlab.com/recognai-team/biome/biome.explore/-/issues/1) for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/biome/lib/python3.7/site-packages/ray/tune/utils/util.py:7: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import defaultdict, deque, Mapping, Sequence\n",
      "/usr/local/anaconda3/envs/biome/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/usr/local/anaconda3/envs/biome/lib/python3.7/site-packages/nltk/lm/vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Counter, Iterable\n",
      "Using custom data configuration default\n",
      "Reusing dataset csv (/Users/frascuchon/.cache/huggingface/datasets/csv/default-1617f6c5fc944723/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Physiotherapie</td>\n",
       "      <td>Bilitza Physiotherapie Benecke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unternehmensberatungen</td>\n",
       "      <td>Kempke Unternehmensberatung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tiefbau</td>\n",
       "      <td>Jürgen Kremer Sietower Bauunternehmen Gmbh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vereine</td>\n",
       "      <td>Kolping Bildungswerk In Der Diözese Augsburg E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vereine</td>\n",
       "      <td>Mittendrin Lübeck E.v.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unternehmensberatungen</td>\n",
       "      <td>Future Consulting Gmbh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Unternehmensberatungen</td>\n",
       "      <td>Fisseler Consulting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maler</td>\n",
       "      <td>Gerhard Kube Lagoni Malereibetrieb Gmbh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Gaststätten, Restaurants - Restaurant Hermes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hotels</td>\n",
       "      <td>Hotels - Hotel Sonnenbichl Fam. Fügenschuh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    label                                               text\n",
       "0          Physiotherapie                     Bilitza Physiotherapie Benecke\n",
       "1  Unternehmensberatungen                        Kempke Unternehmensberatung\n",
       "2                 Tiefbau         Jürgen Kremer Sietower Bauunternehmen Gmbh\n",
       "3                 Vereine  Kolping Bildungswerk In Der Diözese Augsburg E...\n",
       "4                 Vereine                             Mittendrin Lübeck E.v.\n",
       "5  Unternehmensberatungen                             Future Consulting Gmbh\n",
       "6  Unternehmensberatungen                                Fisseler Consulting\n",
       "7                   Maler            Gerhard Kube Lagoni Malereibetrieb Gmbh\n",
       "8             Restaurants       Gaststätten, Restaurants - Restaurant Hermes\n",
       "9                  Hotels         Hotels - Hotel Sonnenbichl Fam. Fügenschuh"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we will load a fake dataset used for our use cases\n",
    "# We'll use the biome.text.Dataset for dataset handling\n",
    "\n",
    "from biome.text import Pipeline, Dataset\n",
    "\n",
    "fake_ds = Dataset.from_csv(\"datasets/business.cat.valid.csv\")\n",
    "fake_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The I'll configure a base client for API comunication\n",
    "\n",
    "from rubrix.sdk import Client, AuthenticatedClient\n",
    "from rubrix.sdk.api.text_classification import bulk_records, search_records\n",
    "from rubrix.sdk.models import *\n",
    "\n",
    "# api_url = \"https://observe-dev.biome.recogn.ai\"\n",
    "api_url = \"http://127.0.0.1:8000\"\n",
    "api_key = \"ab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(base_url=api_url)\n",
    "client = AuthenticatedClient(\n",
    "    base_url=api_url, \n",
    "    token=api_key,\n",
    "    timeout=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "# I have whatever prediction pipeline reciving a text input and geneerate classes and probabilities arrays\n",
    "def predict(text: str):\n",
    "    return [\"A\", \"B\"], [0.9, 0.1]\n",
    "\n",
    "\n",
    "def record_from_data(idx: int, data:dict, prediction:Optional[tuple]=None, annotate:bool=True) -> TextClassificationRecord:\n",
    "    \n",
    "    record = {\n",
    "        \"id\": idx,\n",
    "        \"inputs\": { \"text\" : data[\"text\"]},\n",
    "        \"metadata\": { \"gold\": data[\"label\"], \"input\": data[\"text\"]},\n",
    "    }\n",
    "    \n",
    "    if annotate:\n",
    "        record.update({\n",
    "            \"annotation\": {\n",
    "                \"agent\": \"test_ds\",\n",
    "                \"labels\": [{\"class\": data[\"label\"]}]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    if prediction is not None:\n",
    "        record.update({\n",
    "            \"prediction\": {\n",
    "               \"agent\": \"predict(text:str)\",\n",
    "               \"labels\": [{\"class\":_class, \"confidence\": confidence} for _class, confidence in zip(*prediction)]\n",
    "           }\n",
    "        })\n",
    "    \n",
    "    return TextClassificationRecord.from_dict(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model debugging and development\n",
    "\n",
    "### *I have a trained model and I want to explore its predictions with a test dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(status_code=401, content=b'{\"detail\":\"Could not validate credentials\"}', headers=Headers({'date': 'Wed, 03 Mar 2021 13:23:19 GMT', 'server': 'uvicorn', 'www-authenticate': 'Bearer', 'content-length': '43', 'content-type': 'application/json'}), parsed=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bulk_records.sync_detailed(client=client, json_body=TextClassificationRecordsBulk(\n",
    "    name=\"explore-predictions-test-ds\", \n",
    "    records=[record_from_data(idx, data, predict(data[\"text\"]), annotate=False) for idx, data in enumerate(fake_ds)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_records.sync(client=client, dataset_id=\"explor-predictions-test-ds\", json_body=TextClassificationQuery())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model monitoring and observability\n",
    "\n",
    "### *I am serving a model and I want to log my prediction into a central place.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(status_code=200, content=b'{\"dataset\":\"serving-fake-model-predictions-logs\",\"processed\":2000,\"failed\":0}', headers=Headers({'date': 'Tue, 02 Mar 2021 21:09:13 GMT', 'server': 'uvicorn', 'content-length': '77', 'content-type': 'application/json'}), parsed=BulkResponse(dataset='serving-fake-model-predictions-logs', processed=2000, failed=0, additional_properties={}))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bulk_records.sync_detailed(client=client, json_body=TextClassificationRecordsBulk(\n",
    "    name=\"serving-fake-model-predictions-logs\", \n",
    "    records=[record_from_data(idx, data, predict(data[\"text\"])) for idx, data in enumerate(fake_ds)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BodySearchRecordsClassificationDatasets_DatasetId__SearchPost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-863f7ca241e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBodySearchRecordsClassificationDatasets_DatasetId__SearchPost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"predicted_as\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sort\"\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"by\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"annotated_as\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"order\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"asc\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"serving-fake-model-predictions-logs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BodySearchRecordsClassificationDatasets_DatasetId__SearchPost' is not defined"
     ]
    }
   ],
   "source": [
    "body = BodySearchRecordsClassificationDatasets_DatasetId__SearchPost.from_dict({\"query\": {\"predicted_as\": [\"A\"] }, \"sort\" :[{\"by\": \"annotated_as\", \"order\":\"asc\"}]})\n",
    "search = search_records.sync(client=client, dataset_id=\"serving-fake-model-predictions-logs\", json_body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.records[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8df33d8e207d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *I want to manually provide annotations over these predictions to extract metrics (e.g., production accuracy).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bf4c7e3508bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We fetch the firt data record and include its annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "# We fetch the firt data record and include its annotation\n",
    "\n",
    "record = search.records[0]\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the same record id assures update the same record without insertions\n",
    "\n",
    "bulk_records.sync(client=client, json_body=TextClassificationRecordsBulk(\n",
    "    name=\"serving-fake-model-predictions-logs\", \n",
    "    records=[record_from_data(idx=record.id, data={**record.inputs.to_dict(), \"label\": \"Health\"})]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can confirm the created annotation\n",
    "body =BodySearchRecordsClassificationDatasets_DatasetId__SearchPost(query=TextClassificationQuery(annotated_as=[\"Health\"]))\n",
    "search_records.sync(client=client, dataset_id=\"serving-fake-model-predictions-logs\", json_body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation\n",
    "\n",
    "### *I am starting a model from scratch for a new project and I want to manually label training examples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(status_code=200, content=b'{\"dataset\":\"fake-annotation-session\",\"processed\":2000,\"failed\":0}', headers=Headers({'date': 'Tue, 02 Mar 2021 21:09:30 GMT', 'server': 'uvicorn', 'content-length': '65', 'content-type': 'application/json'}), parsed=BulkResponse(dataset='fake-annotation-session', processed=2000, failed=0, additional_properties={}))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bulk_records.sync_detailed(client=client, json_body=TextClassificationRecordsBulk(\n",
    "    name=\"fake-annotation-session\",\n",
    "    records=[record_from_data(idx, data, annotate=False) for idx, data in enumerate(fake_ds)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = search_records.sync(client=client, dataset_id=\"fake-annotation-session\", json_body=TextClassificationQuery())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.aggregations.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset json (/Users/frascuchon/.cache/huggingface/datasets/json/default-4bed7bf9261a40a7/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[play, Fereydoun, Farrokhzad, best, track]</td>\n",
       "      <td>[O, B-artist, I-artist, B-sort, B-music_item]</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Find, what, movies, are, showing, at, the, ne...</td>\n",
       "      <td>[O, O, B-movie_type, O, O, O, O, B-spatial_rel...</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Will, it, be, chillier, in, La, Mesa, ?]</td>\n",
       "      <td>[O, O, O, B-condition_temperature, O, B-city, ...</td>\n",
       "      <td>GetWeather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[add, sam, sparro, to, my, playlist, called, B...</td>\n",
       "      <td>[O, B-artist, I-artist, O, B-playlist_owner, O...</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Can, I, hear, a, Da, Brat, ep, ?]</td>\n",
       "      <td>[O, O, O, O, B-artist, I-artist, B-music_item, O]</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Rate, Equal, Affections, one, points]</td>\n",
       "      <td>[O, B-object_name, I-object_name, B-rating_val...</td>\n",
       "      <td>RateBook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[What, is, the, Wanda, Group, movie, schedules]</td>\n",
       "      <td>[O, O, O, B-location_name, I-location_name, B-...</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Play, some, theme, songs, from, the, fourties]</td>\n",
       "      <td>[O, O, B-music_item, O, O, O, B-year]</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Include, Sean, Yseult, in, kaitlin's, metal, ...</td>\n",
       "      <td>[O, B-artist, I-artist, O, B-playlist_owner, B...</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Can, you, add, danny, carey, to, my, masters,...</td>\n",
       "      <td>[O, O, O, B-artist, I-artist, O, B-playlist_ow...</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0         [play, Fereydoun, Farrokhzad, best, track]   \n",
       "1  [Find, what, movies, are, showing, at, the, ne...   \n",
       "2          [Will, it, be, chillier, in, La, Mesa, ?]   \n",
       "3  [add, sam, sparro, to, my, playlist, called, B...   \n",
       "4                 [Can, I, hear, a, Da, Brat, ep, ?]   \n",
       "5             [Rate, Equal, Affections, one, points]   \n",
       "6    [What, is, the, Wanda, Group, movie, schedules]   \n",
       "7    [Play, some, theme, songs, from, the, fourties]   \n",
       "8  [Include, Sean, Yseult, in, kaitlin's, metal, ...   \n",
       "9  [Can, you, add, danny, carey, to, my, masters,...   \n",
       "\n",
       "                                              labels                 intent  \n",
       "0      [O, B-artist, I-artist, B-sort, B-music_item]              PlayMusic  \n",
       "1  [O, O, B-movie_type, O, O, O, O, B-spatial_rel...   SearchScreeningEvent  \n",
       "2  [O, O, O, B-condition_temperature, O, B-city, ...             GetWeather  \n",
       "3  [O, B-artist, I-artist, O, B-playlist_owner, O...          AddToPlaylist  \n",
       "4  [O, O, O, O, B-artist, I-artist, B-music_item, O]              PlayMusic  \n",
       "5  [O, B-object_name, I-object_name, B-rating_val...               RateBook  \n",
       "6  [O, O, O, B-location_name, I-location_name, B-...   SearchScreeningEvent  \n",
       "7              [O, O, B-music_item, O, O, O, B-year]              PlayMusic  \n",
       "8  [O, B-artist, I-artist, O, B-playlist_owner, B...          AddToPlaylist  \n",
       "9  [O, O, O, B-artist, I-artist, O, B-playlist_ow...          AddToPlaylist  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from biome.text import Dataset\n",
    "\n",
    "ner_ds = Dataset.from_json(\"datasets/token_classifier.valid.json\")\n",
    "ner_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'album',\n",
       " 'artist',\n",
       " 'best_rating',\n",
       " 'city',\n",
       " 'condition_description',\n",
       " 'condition_temperature',\n",
       " 'country',\n",
       " 'cuisine',\n",
       " 'current_location',\n",
       " 'entity_name',\n",
       " 'facility',\n",
       " 'genre',\n",
       " 'geographic_poi',\n",
       " 'location_name',\n",
       " 'movie_name',\n",
       " 'movie_type',\n",
       " 'music_item',\n",
       " 'object_location_type',\n",
       " 'object_name',\n",
       " 'object_part_of_series_type',\n",
       " 'object_select',\n",
       " 'object_type',\n",
       " 'party_size_description',\n",
       " 'party_size_number',\n",
       " 'playlist',\n",
       " 'playlist_owner',\n",
       " 'poi',\n",
       " 'rating_unit',\n",
       " 'rating_value',\n",
       " 'restaurant_name',\n",
       " 'restaurant_type',\n",
       " 'served_dish',\n",
       " 'service',\n",
       " 'sort',\n",
       " 'spatial_relation',\n",
       " 'state',\n",
       " 'timeRange',\n",
       " 'track',\n",
       " 'year'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {tag[2:] for tags in ner_ds[\"labels\"] for tag in tags if tag != \"O\"}\n",
    "print(\"number of labels:\", len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.sdk.api.token_classification import bulk_records, search_records\n",
    "from rubrix.sdk import Client, AuthenticatedClient\n",
    "from rubrix.sdk.models import *\n",
    "\n",
    "from spacy.gold import offsets_from_biluo_tags, iob_to_biluo\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuthenticatedClient(base_url='http://127.0.0.1:8000', cookies={}, headers={}, timeout=10, token='ab')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# api_url = \"https://observe-dev.biome.recogn.ai\"\n",
    "# api_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJAcmVjb2duYWkiLCJleHAiOjE2MTQ0NTgzNjl9.PlS29RTTrPMKz0FIWO4Qwk_9U_i1q5ZC_OVHbDqRIaU\"\n",
    "\n",
    "local_client = Client(base_url=\"http://localhost:8000\")\n",
    "client = AuthenticatedClient(\n",
    "    base_url=api_url, \n",
    "    token=api_key,\n",
    "    timeout=10\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_record_from_data(idx: int, data:dict, annotate:bool=True) -> TokenClassificationRecord:\n",
    "    \n",
    "    record = {\n",
    "        \"id\": idx,\n",
    "        \"tokens\": data[\"text\"],\n",
    "        \"metadata\": { \"intent\": data[\"intent\"], \"tags\": data[\"labels\"] },\n",
    "    }\n",
    "    \n",
    "    if annotate:\n",
    "        doc = nlp(\" \".join(data[\"text\"]))\n",
    "        record.update({\n",
    "            \"annotation\": {\n",
    "                \"agent\": \"test_ds\",\n",
    "                \"entities\": [{\"start\": start, \"end\": end, \"label\": label} for start, end, label in offsets_from_biluo_tags(doc, iob_to_biluo(data[\"labels\"]))]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return TokenClassificationRecord.from_dict(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3e63622db4b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m bulk_records.sync_detailed(client=client, json_body=TokenClassificationRecordsBulk(\n\u001b[1;32m      2\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"explore-predictions-ner-ds\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mner_record_from_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner_ds' is not defined"
     ]
    }
   ],
   "source": [
    "bulk_records.sync_detailed(client=client, json_body=TokenClassificationRecordsBulk(\n",
    "    name=\"explore-predictions-ner-ds\", \n",
    "    records=[ner_record_from_data(idx, data) for idx, data in enumerate(ner_ds)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
